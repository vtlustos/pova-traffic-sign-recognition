{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Mapillary Traffic Sign Dataset for Detection and Classification on a Global Scale - paper review\n",
    "\n",
    "## Challenges\n",
    "- traffic signs are easily confused with other object classes in street\n",
    "- reflection, low light condition, damages, and occlusion\n",
    "- fine–grained classification\n",
    "- traffic signs are relatively small in size\n",
    "\n",
    "## Dataset Statistics\n",
    "- images: 52,453 fully-anotated 47,547 partialy-anotated\n",
    "- sign categories: 313 + 1 (other sign)\n",
    "- total signs: 257 543\n",
    "\n",
    "| train  | dev   | test   |\n",
    "|--------|-------|--------|\n",
    "| 36 589 | 5 320 | 10 544 |\n",
    "\n",
    "- distribution plots are present in the paper\n",
    "\n",
    "##  Annotation Process \n",
    "The annotations were done by 15 experts trained on this task. The authors continuously controlled the quality of annotations. At least two annotators must have seen each image. To further validate the quality of annotations, they runed separate annotation experiment over smaller subset of images and cross-checked the results showing only minor differences.\n",
    "\n",
    "### 1. Selection\n",
    "The images were selected using the following criteria:\n",
    "- uniform geographical distribution of images around the world (weighted by continent population)\n",
    "- to cover images of different quality, captured under varying conditions\n",
    "- to include as many signs as possible per image\n",
    "- to compensate for the long-tailed distribution of potential traffic\n",
    "sign classes\n",
    "\n",
    "### 2. Annotation\n",
    "The annotation pipeline consisted of 3 steps:\n",
    "1. Image Approval: the annotators should have ensured that the data fulfil the dataset criteria since the pre-selection was automatically\n",
    "2. Sign Localization: The bounding boxes were pre-generated automatically. The annotators were asked to verify and adjust the bounding boxes to fit all traffic signs in the image.\n",
    "3. Sign Classification: The annotators were asked to provide a correct class label for show sign (determined by box). This was not trivial since they used 313 classes. Thereby, the signs were pre-annotated automatically using a proposal network.\n",
    "\n",
    "## Baseline\n",
    "- Faster R-CNN with ResNet50 and ResNet101 back-bones\n",
    "- two tasks: detection only and detection + classification\n",
    "- ResNet50: 83.4 mAP over all 313 classes\n",
    "- their best performing approach used 2 stage pipeline: 1. binary object detection, 2. multi-class classification using a decoupled shallow classification network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"C:\\\\Users\\\\tlust\\\\Downloads\\\\mtsd\"\n",
    "DOUBLE_STEP = False\n",
    "\n",
    "# input files\n",
    "splits_path = os.path.join(PATH, \"splits\")\n",
    "images_path = os.path.join(PATH, \"images\")\n",
    "annotations_path = os.path.join(PATH, \"annotations\")\n",
    "\n",
    "# output files\n",
    "detect_path = os.path.join(PATH, \"yolov8\", \"detect\")\n",
    "coco_path = os.path.join(PATH, \"coco\")\n",
    "cls_path = os.path.join(PATH, \"yolov8\", \"classify\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YOLOv8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detection dataset\n",
    "For a single-step process, classify directly into the full taxonomy; if not, employ binary detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing train split...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 36589/36589 [01:21<00:00, 447.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing val split...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5320/5320 [00:12<00:00, 441.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing test split...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10544/10544 [00:00<00:00, 16688.36it/s]\n"
     ]
    }
   ],
   "source": [
    "labels = []\n",
    "\n",
    "# statistics\n",
    "rejected = 0\n",
    "total = 0\n",
    "sign_distr = {}\n",
    "\n",
    "for split in ['train', 'val']:\n",
    "    print(\"Processing {} split...\".format(split))\n",
    "\n",
    "    # 0. create output directories if not exists\n",
    "    out_dir = os.path.join(detect_path, split)\n",
    "    if not os.path.exists(out_dir):\n",
    "        os.makedirs(os.path.join(out_dir, \"images\"))\n",
    "        os.makedirs(os.path.join(out_dir, \"labels\"))\n",
    "\n",
    "    with open(os.path.join(splits_path, split + \".txt\")) as f:\n",
    "        ids = f.readlines()\n",
    "\n",
    "    for id in tqdm(ids, total=len(ids)):\n",
    "        total += 1    \n",
    "\n",
    "        # 1. set and validate paths\n",
    "        id = id.strip()\n",
    "        img_path = os.path.join(images_path, f\"{id}.jpg\")\n",
    "        ann_path = os.path.join(annotations_path, f\"{id}.json\")\n",
    "        out_img_path = os.path.join(out_dir, \"images\", f\"{id}.jpg\")\n",
    "        out_ann_path = os.path.join(out_dir, \"labels\", f\"{id}.txt\")  \n",
    "\n",
    "        # 1.2. skip if image or annotation does not exists\n",
    "        if (not os.path.exists(img_path)) or (not os.path.exists(ann_path)):\n",
    "            rejected += 1\n",
    "            continue\n",
    "\n",
    "        # 2. copy the image\n",
    "        shutil.copy(img_path, out_img_path)\n",
    "\n",
    "        # 3. create YOLOv8 annotation\n",
    "        with open(ann_path, 'r') as f:\n",
    "            ann = json.load(f)\n",
    "            \n",
    "        with open(out_ann_path, \"a\") as f:  \n",
    "            for obj in ann['objects']:\n",
    "                # 3.1 get label index\n",
    "                if DOUBLE_STEP:\n",
    "                    obj['label'] = 'traffic-sign'\n",
    "                    # in case of double detect only most general label\n",
    "                    #cat = obj['label'].split('--')\n",
    "                    #if len(cat) == 1 and cat[0] == 'other-sign':\n",
    "                    #    continue\n",
    "                    #obj['label'] = f\"{cat[0]}--{cat[1]}\"\n",
    "                else:\n",
    "                    # remove --gN suffix\n",
    "                    obj['label'] = '--'.join(obj['label'].split('--')[:-1])\n",
    "\n",
    "                        \n",
    "                    \n",
    "                if obj['label'] not in labels:\n",
    "                    labels.append(obj['label'])\n",
    "                label = labels.index(obj['label'])\n",
    "\n",
    "                # 3.2 set sign distribution\n",
    "                sign_distr[label] = sign_distr[label] + 1 if label in sign_distr else 1\n",
    "\n",
    "                # 3.3 get bounding box\n",
    "                bbox = obj['bbox']\n",
    "                x_center = np.clip(((bbox['xmin'] + bbox['xmax']) / 2) / ann['width'], 0, 1)\n",
    "                y_center = np.clip(((bbox['ymin'] + bbox['ymax'] ) / 2) / ann['height'], 0, 1)\n",
    "                width = np.clip((bbox['xmax'] - bbox['xmin']) / ann['width'], 0, 1)\n",
    "                height = np.clip((bbox['ymax'] - bbox['ymin']) / ann['height'], 0, 1)\n",
    "                obj_ann = f\"{label} {x_center} {y_center} {width} {height} \\n\"\n",
    "\n",
    "                # 3.4 write annotation\n",
    "                if width > 0 and height > 0:\n",
    "                    f.write(obj_ann)\n",
    "\n",
    "# 4. create dataset.yaml\n",
    "with open(os.path.join(detect_path, \"dataset.yaml\"), \"a\") as f:\n",
    "    f.write(\"path: {detect_path}\\n\")\n",
    "    f.write(f\"train: {os.path.join('train', 'images')}\\n\")\n",
    "    f.write(f\"val: {os.path.join('val', 'images')}\\n\")\n",
    "    f.write(f\"names:\\n\")\n",
    "    for ix, label in enumerate(labels):\n",
    "        f.write(f\"  {ix}: {label}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification dataset\n",
    "If a two-stage pipeline is chosen, then construct a classification dataset that exclusively includes extracted signs, sorted by their respective labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing train split...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 36589/36589 [29:34<00:00, 20.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing val split...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5320/5320 [04:20<00:00, 20.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing test split...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10544/10544 [00:00<00:00, 20023.83it/s]\n"
     ]
    }
   ],
   "source": [
    "if DOUBLE_STEP:\n",
    "    labels = []\n",
    "\n",
    "    # statistics\n",
    "    rejected = 0\n",
    "    total = 0\n",
    "    sign_distr = {}\n",
    "\n",
    "\n",
    "    for split in ['train', 'val', 'test']:\n",
    "        print(\"Processing {} split...\".format(split))\n",
    "   \n",
    "        with open(os.path.join(splits_path, split + \".txt\")) as f:\n",
    "            ids = f.readlines()\n",
    "\n",
    "        for id in tqdm(ids, total=len(ids)):\n",
    "            total += 1\n",
    "\n",
    "            # 1. set and validate paths\n",
    "            id = id.strip()\n",
    "            img_path = os.path.join(images_path, f\"{id}.jpg\")\n",
    "            ann_path = os.path.join(annotations_path, f\"{id}.json\")\n",
    "    \n",
    "            # 1.2. skip if image or annotation does not exists\n",
    "            if (not os.path.exists(img_path)) or (not os.path.exists(ann_path)):\n",
    "                rejected += 1\n",
    "                continue\n",
    "\n",
    "            # 2. load the image\n",
    "            img = cv2.imread(img_path)\n",
    "\n",
    "            # 3. extract traffic sign and create classification dataset                \n",
    "            with open(ann_path, 'r') as f:\n",
    "                ann = json.load(f)\n",
    "\n",
    "            for obj in ann['objects']:\n",
    "                # 3.1 skip if other-sign\n",
    "                if obj['label'] == 'other-sign':\n",
    "                    continue\n",
    "                # remove --gN suffix\n",
    "                obj['label'] = '--'.join(obj['label'].split('--')[:-1])\n",
    "\n",
    "                # 3.2 get sign path and create directory if not exists\n",
    "                sign_dir = os.path.join(cls_path, split, obj['label'])\n",
    "                if not os.path.exists(sign_dir):\n",
    "                    os.makedirs(sign_dir)\n",
    "\n",
    "                # 3.3 increment sign counter\n",
    "                if obj['label'] not in labels:\n",
    "                    labels.append(obj['label'])\n",
    "                label = labels.index(obj['label'])            \n",
    "                sign_distr[label] = sign_distr[label] + 1 if label in sign_distr else 1\n",
    "\n",
    "                # 3.4 get bounding box\n",
    "                sign = img[\n",
    "                    int(obj['bbox']['ymin']):int(obj['bbox']['ymax']), \n",
    "                    int(obj['bbox']['xmin']):int(obj['bbox']['xmax'])\n",
    "                ]\n",
    "\n",
    "                # 3.4 save sign\n",
    "                if sign.shape[0] > 0 and sign.shape[1] > 0 and sign.shape[2] > 0:\n",
    "                    sign_path = os.path.join(sign_dir, f\"{id}_{obj['key']}.jpg\")\n",
    "                    cv2.imwrite(sign_path, sign)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. view dataset statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Images - total: {}\".format(total))\n",
    "print(\"Images - rejected: {}\".format(rejected) + \" ({:.2f}%)\".format(rejected / total * 100))\n",
    "print(\"Signs: {}\".format(np.sum(sign_distr.values())))\n",
    "for ix, value in sign_distr.items():\n",
    "    print(f\"{labels[ix]}: {value}\")\n",
    "\n",
    "# remove 'other-label' from statistics\n",
    "sign_distr_cpy = sign_distr.copy()\n",
    "#sign_distr_cpy[1] = 0 # other-label\n",
    "plt.bar(labels, sign_distr_cpy.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COCO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting globox\n",
      "  Obtaining dependency information for globox from https://files.pythonhosted.org/packages/50/6b/85af78fd335b8c232f8cb201b214e11d2dedc5041ed780226b845a2510a9/globox-2.4.2-py3-none-any.whl.metadata\n",
      "  Downloading globox-2.4.2-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting numpy<2.0.0,>=1.26.0 (from globox)\n",
      "  Obtaining dependency information for numpy<2.0.0,>=1.26.0 from https://files.pythonhosted.org/packages/07/34/748ec8c81235277f62cc04488052fe28b8b69280e7275bbb8dc143cd7791/numpy-1.26.2-cp39-cp39-win_amd64.whl.metadata\n",
      "  Downloading numpy-1.26.2-cp39-cp39-win_amd64.whl.metadata (61 kB)\n",
      "     ---------------------------------------- 0.0/61.2 kB ? eta -:--:--\n",
      "     ------------------------- ------------ 41.0/61.2 kB 991.0 kB/s eta 0:00:01\n",
      "     ---------------------------------------- 61.2/61.2 kB 1.1 MB/s eta 0:00:00\n",
      "Collecting rich<14.0.0,>=13.3.5 (from globox)\n",
      "  Obtaining dependency information for rich<14.0.0,>=13.3.5 from https://files.pythonhosted.org/packages/be/be/1520178fa01eabe014b16e72a952b9f900631142ccd03dc36cf93e30c1ce/rich-13.7.0-py3-none-any.whl.metadata\n",
      "  Downloading rich-13.7.0-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.65.0 in c:\\users\\tlust\\anaconda3\\envs\\torch\\lib\\site-packages (from globox) (4.65.0)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich<14.0.0,>=13.3.5->globox)\n",
      "  Obtaining dependency information for markdown-it-py>=2.2.0 from https://files.pythonhosted.org/packages/42/d7/1ec15b46af6af88f19b8e5ffea08fa375d433c998b8a7639e76935c14f1f/markdown_it_py-3.0.0-py3-none-any.whl.metadata\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\tlust\\anaconda3\\envs\\torch\\lib\\site-packages (from rich<14.0.0,>=13.3.5->globox) (2.16.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\tlust\\anaconda3\\envs\\torch\\lib\\site-packages (from tqdm<5.0.0,>=4.65.0->globox) (0.4.6)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich<14.0.0,>=13.3.5->globox)\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Downloading globox-2.4.2-py3-none-any.whl (33 kB)\n",
      "Downloading numpy-1.26.2-cp39-cp39-win_amd64.whl (15.8 MB)\n",
      "   ---------------------------------------- 0.0/15.8 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.2/15.8 MB 6.1 MB/s eta 0:00:03\n",
      "   -- ------------------------------------- 1.0/15.8 MB 12.5 MB/s eta 0:00:02\n",
      "   ---- ----------------------------------- 1.8/15.8 MB 16.0 MB/s eta 0:00:01\n",
      "   ----- ---------------------------------- 2.1/15.8 MB 12.0 MB/s eta 0:00:02\n",
      "   ------ --------------------------------- 2.5/15.8 MB 11.5 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 3.0/15.8 MB 11.1 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 3.8/15.8 MB 12.9 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 5.2/15.8 MB 14.3 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 6.3/15.8 MB 15.5 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 7.7/15.8 MB 17.0 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 8.7/15.8 MB 17.4 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 10.3/15.8 MB 19.3 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 11.7/15.8 MB 21.1 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 13.2/15.8 MB 27.3 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 14.6/15.8 MB 28.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  15.8/15.8 MB 28.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 15.8/15.8 MB 27.3 MB/s eta 0:00:00\n",
      "Downloading rich-13.7.0-py3-none-any.whl (240 kB)\n",
      "   ---------------------------------------- 0.0/240.6 kB ? eta -:--:--\n",
      "   ---------------------------------------- 240.6/240.6 kB ? eta 0:00:00\n",
      "Downloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "   ---------------------------------------- 0.0/87.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 87.5/87.5 kB ? eta 0:00:00\n",
      "Installing collected packages: numpy, mdurl, markdown-it-py, rich, globox\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.25.2\n",
      "    Uninstalling numpy-1.25.2:\n",
      "      Successfully uninstalled numpy-1.25.2\n",
      "Successfully installed globox-2.4.2 markdown-it-py-3.0.0 mdurl-0.1.2 numpy-1.26.2 rich-13.7.0\n"
     ]
    }
   ],
   "source": [
    "! pip install globox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from globox import AnnotationSet\n",
    "\n",
    "# create dirs if not exists\n",
    "img_dir = os.path.join(coco_path, \"images\")\n",
    "ann_dir = os.path.join(coco_path, \"annotations\")\n",
    "if not os.path.exists(ann_dir) or not os.path.exists(img_dir):\n",
    "    os.makedirs(ann_dir)\n",
    "    os.makedirs(img_dir)\n",
    "\n",
    "for split in ['train', 'val']:\n",
    "    yolo_img_dir = os.path.join(detect_path, split, \"images\")\n",
    "\n",
    "    # copy images\n",
    "    for img_name in os.listdir(yolo_img_dir):\n",
    "        shutil.copy(\n",
    "            os.path.join(yolo_img_dir, img_name),\n",
    "            os.path.join(img_dir, img_name)\n",
    "        )\n",
    "\n",
    "    # convert annotations\n",
    "    yolo = AnnotationSet.from_yolo_v5(\n",
    "        folder=os.path.join(detect_path, split, \"labels\"),\n",
    "        image_folder=yolo_img_dir\n",
    "    )\n",
    "    yolo.save_coco(os.path.join(ann_dir, split + \".json\"), auto_ids=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
