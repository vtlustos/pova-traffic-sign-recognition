{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Mapillary Traffic Sign Dataset for Detection and Classification on a Global Scale - paper review\n",
    "\n",
    "## Challenges\n",
    "- traffic signs are easily confused with other object classes in street\n",
    "- reflection, low light condition, damages, and occlusion\n",
    "- fine–grained classification\n",
    "- traffic signs are relatively small in size\n",
    "\n",
    "## Dataset Statistics\n",
    "- images: 52,453 fully-anotated 47,547 partialy-anotated\n",
    "- sign categories: 313 + 1 (other sign)\n",
    "- total signs: 257 543\n",
    "\n",
    "| train  | dev   | test   |\n",
    "|--------|-------|--------|\n",
    "| 36 589 | 5 320 | 10 544 |\n",
    "\n",
    "- distribution plots are present in the paper\n",
    "\n",
    "##  Annotation Process \n",
    "The annotations were done by 15 experts trained on this task. The authors continuously controlled the quality of annotations. At least two annotators must have seen each image. To further validate the quality of annotations, they runed separate annotation experiment over smaller subset of images and cross-checked the results showing only minor differences.\n",
    "\n",
    "### 1. Selection\n",
    "The images were selected using the following criteria:\n",
    "- uniform geographical distribution of images around the world (weighted by continent population)\n",
    "- to cover images of different quality, captured under varying conditions\n",
    "- to include as many signs as possible per image\n",
    "- to compensate for the long-tailed distribution of potential traffic\n",
    "sign classes\n",
    "\n",
    "### 2. Annotation\n",
    "The annotation pipeline consisted of 3 steps:\n",
    "1. Image Approval: the annotators should have ensured that the data fulfil the dataset criteria since the pre-selection was automatically\n",
    "2. Sign Localization: The bounding boxes were pre-generated automatically. The annotators were asked to verify and adjust the bounding boxes to fit all traffic signs in the image.\n",
    "3. Sign Classification: The annotators were asked to provide a correct class label for show sign (determined by box). This was not trivial since they used 313 classes. Thereby, the signs were pre-annotated automatically using a proposal network.\n",
    "\n",
    "## Baseline\n",
    "- Faster R-CNN with ResNet50 and ResNet101 back-bones\n",
    "- two tasks: detection only and detection + classification\n",
    "- ResNet50: 83.4 mAP over all 313 classes\n",
    "- their best performing approach used 2 stage pipeline: 1. binary object detection, 2. multi-class classification using a decoupled shallow classification network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"/auto/brno12-cerit/nfs4/home/xkotou06/POVa/mtsd\"\n",
    "\n",
    "splits_path = os.path.join(PATH, \"splits\")\n",
    "images_path = os.path.join(PATH, \"images\")\n",
    "annotations_path = os.path.join(PATH, \"annotations\")\n",
    "\n",
    "detect_path = os.path.join(PATH, \"yolov8\", \"detect\")\n",
    "cls_path = os.path.join(PATH, \"yolov8\", \"classify\")\n",
    "coco_path = os.path.join(PATH, \"coco\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YOLOv8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOUBLE_STEP = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detection dataset\n",
    "For a single-step process, classify directly into the full taxonomy; if not, employ binary detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing train split...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 36589/36589 [01:21<00:00, 448.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing val split...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5320/5320 [00:11<00:00, 464.30it/s]\n"
     ]
    }
   ],
   "source": [
    "labels = []\n",
    "\n",
    "# statistics\n",
    "rejected = 0\n",
    "total = 0\n",
    "sign_distr = {}\n",
    "\n",
    "for split in ['train', 'val']:\n",
    "    print(\"Processing {} split...\".format(split))\n",
    "\n",
    "    # 0. create output directories if not exists\n",
    "    out_dir = os.path.join(detect_path, split)\n",
    "    if not os.path.exists(out_dir):\n",
    "        os.makedirs(os.path.join(out_dir, \"images\"))\n",
    "        os.makedirs(os.path.join(out_dir, \"labels\"))\n",
    "\n",
    "    with open(os.path.join(splits_path, split + \".txt\")) as f:\n",
    "        ids = f.readlines()\n",
    "\n",
    "    for id in tqdm(ids, total=len(ids)):\n",
    "        total += 1    \n",
    "\n",
    "        # 1. set and validate paths\n",
    "        id = id.strip()\n",
    "        img_path = os.path.join(images_path, f\"{id}.jpg\")\n",
    "        ann_path = os.path.join(annotations_path, f\"{id}.json\")\n",
    "        out_img_path = os.path.join(out_dir, \"images\", f\"{id}.jpg\")\n",
    "        out_ann_path = os.path.join(out_dir, \"labels\", f\"{id}.txt\")  \n",
    "\n",
    "        # 1.2. skip if image or annotation does not exists\n",
    "        if (not os.path.exists(img_path)) or (not os.path.exists(ann_path)):\n",
    "            rejected += 1\n",
    "            continue\n",
    "\n",
    "        # 2. read annotation \n",
    "        with open(ann_path, 'r') as f:\n",
    "            ann = json.load(f)\n",
    "\n",
    "        # 2.1 skip if panormaic image\n",
    "        if ann['ispano'] == True:\n",
    "            rejected += 1\n",
    "            continue\n",
    "\n",
    "        # 3. create annotation file and copy image\n",
    "        is_empty = True\n",
    "        for obj in ann['objects']:\n",
    "            # 3.1 get label index\n",
    "            if DOUBLE_STEP:\n",
    "                obj['label'] = 'traffic-sign'                    \n",
    "            else:\n",
    "                if obj['label'] == 'other-sign':\n",
    "                    continue\n",
    "                else:\n",
    "                    obj['label'] = '--'.join(obj['label'].split('--')[1:-1])                      \n",
    "                \n",
    "            if obj['label'] not in labels:\n",
    "                labels.append(obj['label'])\n",
    "            label = labels.index(obj['label'])\n",
    "\n",
    "            # 3.2 set sign distribution\n",
    "            sign_distr[label] = sign_distr[label] + 1 if label in sign_distr else 1\n",
    "\n",
    "            # 3.3 get bounding box\n",
    "            bbox = obj['bbox']\n",
    "            x_center = np.clip(((bbox['xmin'] + bbox['xmax']) / 2) / ann['width'], 0, 1)\n",
    "            y_center = np.clip(((bbox['ymin'] + bbox['ymax'] ) / 2) / ann['height'], 0, 1)\n",
    "            width = np.clip((bbox['xmax'] - bbox['xmin']) / ann['width'], 0, 1)\n",
    "            height = np.clip((bbox['ymax'] - bbox['ymin']) / ann['height'], 0, 1)\n",
    "            obj_ann = f\"{label} {x_center} {y_center} {width} {height} \\n\"\n",
    "\n",
    "            # 3.4 write annotation\n",
    "            if width > 0.01 and height > 0.01:\n",
    "                is_empty = False\n",
    "                with open(out_ann_path, \"a\") as f:  \n",
    "                    f.write(obj_ann)\n",
    "            \n",
    "            if is_empty == False:\n",
    "                # 3.5. copy the image\n",
    "                shutil.copy(img_path, out_img_path)\n",
    "            else:\n",
    "                rejected += 1\n",
    "\n",
    "# 4. create dataset.yaml\n",
    "with open(os.path.join(detect_path, \"dataset.yaml\"), \"a\") as f:\n",
    "    f.write(f\"path: {detect_path}\\n\")\n",
    "    f.write(f\"train: {os.path.join('train', 'images')}\\n\")\n",
    "    f.write(f\"val: {os.path.join('val', 'images')}\\n\")\n",
    "    f.write(f\"names:\\n\")\n",
    "    for ix, label in enumerate(labels):\n",
    "        f.write(f\"  {ix}: {label}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification dataset\n",
    "If a two-stage pipeline is chosen, then construct a classification dataset that exclusively includes extracted signs, sorted by their respective labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DOUBLE_STEP:\n",
    "    labels = []\n",
    "\n",
    "    # statistics\n",
    "    rejected = 0\n",
    "    total = 0\n",
    "    sign_distr = {}\n",
    "\n",
    "\n",
    "    for split in ['train', 'val', 'test']:\n",
    "        print(\"Processing {} split...\".format(split))\n",
    "   \n",
    "        with open(os.path.join(splits_path, split + \".txt\")) as f:\n",
    "            ids = f.readlines()\n",
    "\n",
    "        for id in tqdm(ids, total=len(ids)):\n",
    "            total += 1\n",
    "\n",
    "            # 1. set and validate paths\n",
    "            id = id.strip()\n",
    "            img_path = os.path.join(images_path, f\"{id}.jpg\")\n",
    "            ann_path = os.path.join(annotations_path, f\"{id}.json\")\n",
    "    \n",
    "            # 1.2. skip if image or annotation does not exists\n",
    "            if (not os.path.exists(img_path)) or (not os.path.exists(ann_path)):\n",
    "                rejected += 1\n",
    "                continue\n",
    "\n",
    "            # 2. load the image\n",
    "            img = cv2.imread(img_path)\n",
    "\n",
    "            # 3. extract traffic sign and create classification dataset                \n",
    "            with open(ann_path, 'r') as f:\n",
    "                ann = json.load(f)\n",
    "\n",
    "            for obj in ann['objects']:\n",
    "                # 3.1 skip if other-sign\n",
    "                if obj['label'] == 'other-sign':\n",
    "                    continue\n",
    "                # remove --gN suffix\n",
    "                obj['label'] = '--'.join(obj['label'].split('--')[:-1])\n",
    "\n",
    "                # 3.2 get sign path and create directory if not exists\n",
    "                sign_dir = os.path.join(cls_path, split, obj['label'])\n",
    "                if not os.path.exists(sign_dir):\n",
    "                    os.makedirs(sign_dir)\n",
    "\n",
    "                # 3.3 increment sign counter\n",
    "                if obj['label'] not in labels:\n",
    "                    labels.append(obj['label'])\n",
    "                label = labels.index(obj['label'])            \n",
    "                sign_distr[label] = sign_distr[label] + 1 if label in sign_distr else 1\n",
    "\n",
    "                # 3.4 get bounding box\n",
    "                sign = img[\n",
    "                    int(obj['bbox']['ymin']):int(obj['bbox']['ymax']), \n",
    "                    int(obj['bbox']['xmin']):int(obj['bbox']['xmax'])\n",
    "                ]\n",
    "\n",
    "                # 3.4 save sign\n",
    "                if sign.shape[0] > 0 and sign.shape[1] > 0 and sign.shape[2] > 0:\n",
    "                    sign_path = os.path.join(sign_dir, f\"{id}_{obj['key']}.jpg\")\n",
    "                    cv2.imwrite(sign_path, sign)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. view dataset statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Images - total: {}\".format(total))\n",
    "print(\"Images - rejected: {}\".format(rejected) + \" ({:.2f}%)\".format(rejected / total * 100))\n",
    "print(\"Signs: {}\".format(np.sum(sign_distr.values())))\n",
    "for ix, value in sign_distr.items():\n",
    "    print(f\"{labels[ix]}: {value}\")\n",
    "plt.bar(labels, sign_distr.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COCO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install globox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from globox import AnnotationSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dirs if not exists\n",
    "img_dir = os.path.join(coco_path, \"images\")\n",
    "ann_dir = os.path.join(coco_path, \"annotations\")\n",
    "if not os.path.exists(ann_dir) or not os.path.exists(img_dir):\n",
    "    os.makedirs(ann_dir)\n",
    "    os.makedirs(img_dir)\n",
    "\n",
    "for split in ['train', 'val']:\n",
    "    yolo_img_dir = os.path.join(detect_path, split, \"images\")\n",
    "\n",
    "    # copy images\n",
    "    for img_name in os.listdir(yolo_img_dir):\n",
    "        shutil.copy(\n",
    "            os.path.join(yolo_img_dir, img_name),\n",
    "            os.path.join(img_dir, img_name)\n",
    "        )\n",
    "\n",
    "    # convert annotations\n",
    "    yolo = AnnotationSet.from_yolo_v5(\n",
    "        folder=os.path.join(detect_path, split, \"labels\"),\n",
    "        image_folder=yolo_img_dir\n",
    "    )\n",
    "    yolo.save_coco(os.path.join(ann_dir, split + \".json\"), auto_ids=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
