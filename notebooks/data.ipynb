{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Mapillary Traffic Sign Dataset for Detection and Classification on a Global Scale - paper review\n",
    "\n",
    "## Challenges\n",
    "- traffic signs are easily confused with other object classes in street\n",
    "- reflection, low light condition, damages, and occlusion\n",
    "- fineâ€“grained classification\n",
    "- traffic signs are relatively small in size\n",
    "\n",
    "## Dataset Statistics\n",
    "- images: 52,453 fully-anotated 47,547 partialy-anotated\n",
    "- sign categories: 313 + 1 (other sign)\n",
    "- total signs: 257 543\n",
    "\n",
    "| train  | dev   | test   |\n",
    "|--------|-------|--------|\n",
    "| 36 589 | 5 320 | 10 544 |\n",
    "\n",
    "- distribution plots are present in the paper\n",
    "\n",
    "##  Annotation Process \n",
    "The annotations were done by 15 experts trained on this task. The authors continuously controlled the quality of annotations. At least two annotators must have seen each image. To further validate the quality of annotations, they runed separate annotation experiment over smaller subset of images and cross-checked the results showing only minor differences.\n",
    "\n",
    "### 1. Selection\n",
    "The images were selected using the following criteria:\n",
    "- uniform geographical distribution of images around the world (weighted by continent population)\n",
    "- to cover images of different quality, captured under varying conditions\n",
    "- to include as many signs as possible per image\n",
    "- to compensate for the long-tailed distribution of potential traffic\n",
    "sign classes\n",
    "\n",
    "### 2. Annotation\n",
    "The annotation pipeline consisted of 3 steps:\n",
    "1. Image Approval: the annotators should have ensured that the data fulfil the dataset criteria since the pre-selection was automatically\n",
    "2. Sign Localization: The bounding boxes were pre-generated automatically. The annotators were asked to verify and adjust the bounding boxes to fit all traffic signs in the image.\n",
    "3. Sign Classification: The annotators were asked to provide a correct class label for show sign (determined by box). This was not trivial since they used 313 classes. Thereby, the signs were pre-annotated automatically using a proposal network.\n",
    "\n",
    "## Baseline\n",
    "- Faster R-CNN with ResNet50 and ResNet101 back-bones\n",
    "- two tasks: detection only and detection + classification\n",
    "- ResNet50: 83.4 mAP over all 313 classes\n",
    "- their best performing approach used 2 stage pipeline: 1. binary object detection, 2. multi-class classification using a decoupled shallow classification network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"C:\\\\Users\\\\tlust\\\\Downloads\\\\mtsd\"\n",
    "\n",
    "# input files\n",
    "splits_path = os.path.join(PATH, \"splits\")\n",
    "images_path = os.path.join(PATH, \"images\")\n",
    "annotations_path = os.path.join(PATH, \"annotations\")\n",
    "\n",
    "# output files\n",
    "output_path = os.path.join(PATH, \"yolov8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YOLOv8 parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "\n",
    "# statistics\n",
    "rejected = 0\n",
    "total = 0\n",
    "sign_distr = {}\n",
    "\n",
    "for split in ['train', 'val', 'test']:\n",
    "    print(\"Processing {} split...\".format(split))\n",
    "\n",
    "    # 0. create output directories if not exists\n",
    "    out_dir = os.path.join(output_path, split)\n",
    "    if not os.path.exists(out_dir):\n",
    "        os.makedirs(os.path.join(out_dir, \"images\"))\n",
    "        os.makedirs(os.path.join(out_dir, \"labels\"))\n",
    "\n",
    "    with open(os.path.join(splits_path, split + \".txt\")) as f:\n",
    "        ids = f.readlines()\n",
    "\n",
    "    for id in tqdm(ids, total=len(ids)):\n",
    "        total += 1    \n",
    "\n",
    "        # 1. set and validate paths\n",
    "        id = id.strip()\n",
    "        img_path = os.path.join(images_path, f\"{id}.jpg\")\n",
    "        ann_path = os.path.join(annotations_path, f\"{id}.json\")\n",
    "        out_img_path = os.path.join(out_dir, \"images\", f\"{id}.jpg\")\n",
    "        out_ann_path = os.path.join(out_dir, \"labels\", f\"{id}.txt\")  \n",
    "  \n",
    "        # 1.2. skip if image or annotation does not exists\n",
    "        if (not os.path.exists(img_path)) or (not os.path.exists(ann_path)):\n",
    "            rejected += 1\n",
    "            continue\n",
    "\n",
    "        # 2. copy the image\n",
    "        shutil.copy(img_path, out_img_path)\n",
    "\n",
    "        # 3. create YOLOv8 annotation\n",
    "        with open(ann_path, 'r') as f:\n",
    "            ann = json.load(f)\n",
    "            \n",
    "        with open(out_ann_path, \"a\") as f:  \n",
    "            for obj in ann['objects']:\n",
    "                # 3.1 get label index\n",
    "                if obj['label'] not in labels:\n",
    "                    labels.append(obj['label'])\n",
    "                label = labels.index(obj['label'])\n",
    "\n",
    "                # 3.2 set sign distribution\n",
    "                sign_distr[label] = sign_distr[label] + 1 if label in sign_distr else 1\n",
    "\n",
    "                # 3.3 get bounding box\n",
    "                bbox = obj['bbox']\n",
    "                x_center = ((bbox['xmin'] + bbox['xmax']) / 2) / ann['width']\n",
    "                y_center = ((bbox['ymin'] + bbox['ymax'] ) / 2) / ann['height']\n",
    "                width = (bbox['xmax'] - bbox['xmin']) / ann['width']\n",
    "                height = (bbox['ymax'] - bbox['ymin']) / ann['height']\n",
    "                obj_ann = f\"{label} {x_center} {y_center} {width} {height} \\n\"\n",
    "\n",
    "                # 3.4 write annotation\n",
    "                f.write(obj_ann)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Images - total: {}\".format(total))\n",
    "print(\"Images - rejected: {}\".format(rejected) + \" ({:.2f}%)\".format(rejected / total * 100))\n",
    "print(\"Signs: {}\".format(np.sum(sign_distr.values())))\n",
    "plt.bar(labels, sign_distr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
