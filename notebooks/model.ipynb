{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YOLOv8 training\n",
    "This part of the project explores capabilites of the YOLOv8 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install ultralytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH = \"C:\\\\Users\\\\tlust\\\\Downloads\\\\mtsd\\\\yolov8\"\n",
    "detect_path = os.path.join(BASE_PATH, \"detect\", \"dataset.yaml\")\n",
    "classify_path = os.path.join(BASE_PATH, \"classify\")\n",
    "\n",
    "DOUBLE_STEP = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-step fully taxonomy detection + classification to 313 classes\n",
    "Initial experiment to assess the model's default performance across the entire taxonomy. Anticipated to yield suboptimal results due to the extensive number of classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DOUBLE_STEP == False:\n",
    "    #model = YOLO('yolov8n.yaml')  # build a new model from YAML\n",
    "    #model = YOLO('yolov8n.yaml').load('yolov8n.pt')  # build from YAML and transfer weights\n",
    "    model = YOLO(\"yolov8m.pt\")  # load a pretrained model\n",
    "    results = model.train(data=detect_path, epochs=100, imgsz=640, batch=16, fliplr=0)\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-stage pipeline\n",
    "Anticipated to yield improved outcomes as a result of decoupling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. train binary sign detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DOUBLE_STEP:\n",
    "    #model = YOLO('yolov8n.yaml')                           # build a new model from YAML\n",
    "    #model = YOLO('yolov8n.yaml').load('yolov8n.pt')        # build from YAML and transfer weights\n",
    "    model = YOLO('yolov8n.pt')  # load a pretrained model\n",
    "    results = model.train(data=detect_path, epochs=10, imgsz=640)\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. train sign classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DOUBLE_STEP:\n",
    "    #model = YOLO('yolov8n-cls.yaml')                           # build a new model from YAML\n",
    "    #model = YOLO('yolov8n-cls.yaml').load('yolov8n-cls.pt')    # build from YAML and transfer weights\n",
    "    model = YOLO('yolov8x-cls.pt')  # load a pretrained model\n",
    "    results = model.train(data=classify_path, epochs=100, imgsz=224, batch=128)\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DETR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torchvision\n",
    "\n",
    "class MappilaryDataset(torchvision.datasets.CocoDetection):\n",
    "    def __init__(self, dir, processor, train=True):\n",
    "        super(MappilaryDataset, self).__init__(\n",
    "            os.path.join(dir, \"images\"),\n",
    "            os.path.join(dir, \"annotations\", \"train.json\" if train else \"val.json\")\n",
    "        )\n",
    "        self.processor = processor\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img, target = super(MappilaryDataset, self).__getitem__(idx)\n",
    "        image_id = self.ids[idx]\n",
    "        target = {\n",
    "            'image_id': image_id, \n",
    "            'annotations': target\n",
    "        }\n",
    "        encoding = self.processor(\n",
    "            images=img, \n",
    "            annotations=target, \n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        pixel_values = encoding[\"pixel_values\"].squeeze()\n",
    "        target = encoding[\"labels\"][0]\n",
    "        return pixel_values, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DetrImageProcessor\n",
    "\n",
    "processor = DetrImageProcessor.from_pretrained(\n",
    "    \"facebook/detr-resnet-50\"\n",
    ")\n",
    "train_dataset = MappilaryDataset(\n",
    "    dir='C:/Users/tlust/Downloads/mtsd/coco', \n",
    "    processor=processor\n",
    ")\n",
    "val_dataset = MappilaryDataset(\n",
    "    dir='C:/Users/tlust/Downloads/mtsd/coco', \n",
    "    processor=processor, train=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def collate_fn(batch):\n",
    "  pixel_values = [item[0] for item in batch]\n",
    "  encoding = processor.pad(pixel_values, return_tensors=\"pt\")\n",
    "  labels = [item[1] for item in batch]\n",
    "  batch = {}\n",
    "  batch['pixel_values'] = encoding['pixel_values']\n",
    "  batch['pixel_mask'] = encoding['pixel_mask']\n",
    "  batch['labels'] = labels\n",
    "  return batch\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "  train_dataset, \n",
    "  collate_fn=collate_fn, \n",
    "  batch_size=4, shuffle=True\n",
    ")\n",
    "val_dataloader = DataLoader(\n",
    "  val_dataset, \n",
    "  collate_fn=collate_fn, \n",
    "  batch_size=2\n",
    ")\n",
    "batch = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from transformers import DetrConfig, DetrForObjectDetection\n",
    "import torch\n",
    "\n",
    "class Detr(pl.LightningModule):\n",
    "     def __init__(self, lr, lr_backbone, weight_decay):\n",
    "         super().__init__()\n",
    "         self.model = DetrForObjectDetection.from_pretrained(\n",
    "            \"facebook/detr-resnet-50\",\n",
    "            num_labels=257,\n",
    "            ignore_mismatched_sizes=True\n",
    "         )\n",
    "         self.lr = lr\n",
    "         self.lr_backbone = lr_backbone\n",
    "         self.weight_decay = weight_decay\n",
    "\n",
    "     def forward(self, pixel_values, pixel_mask):\n",
    "       outputs = self.model(pixel_values=pixel_values, pixel_mask=pixel_mask)\n",
    "       return outputs\n",
    "     \n",
    "     def common_step(self, batch, batch_idx):\n",
    "       pixel_values = batch[\"pixel_values\"]\n",
    "       pixel_mask = batch[\"pixel_mask\"]\n",
    "       labels = [{k: v.to(self.device) for k, v in t.items()} for t in batch[\"labels\"]]\n",
    "       outputs = self.model(pixel_values=pixel_values, pixel_mask=pixel_mask, labels=labels)\n",
    "       loss = outputs.loss\n",
    "       loss_dict = outputs.loss_dict\n",
    "       return loss, loss_dict\n",
    "\n",
    "     def training_step(self, batch, batch_idx):\n",
    "        loss, loss_dict = self.common_step(batch, batch_idx)     \n",
    "        self.log(\"training_loss\", loss)\n",
    "        for k,v in loss_dict.items():\n",
    "          self.log(\"train_\" + k, v.item())\n",
    "        return loss\n",
    "\n",
    "     def validation_step(self, batch, batch_idx):\n",
    "        loss, loss_dict = self.common_step(batch, batch_idx)     \n",
    "        self.log(\"validation_loss\", loss)\n",
    "        for k,v in loss_dict.items():\n",
    "          self.log(\"validation_\" + k, v.item())\n",
    "\n",
    "        return loss\n",
    "\n",
    "     def configure_optimizers(self):\n",
    "        param_dicts = [\n",
    "              {\"params\": [p for n, p in self.named_parameters() if \"backbone\" not in n and p.requires_grad]},\n",
    "              {\n",
    "                  \"params\": [p for n, p in self.named_parameters() if \"backbone\" in n and p.requires_grad],\n",
    "                  \"lr\": self.lr_backbone,\n",
    "              },\n",
    "        ]\n",
    "        optimizer = torch.optim.AdamW(param_dicts, lr=self.lr,\n",
    "                                  weight_decay=self.weight_decay)\n",
    "        \n",
    "        return optimizer\n",
    "\n",
    "     def train_dataloader(self):\n",
    "        return train_dataloader\n",
    "\n",
    "     def val_dataloader(self):\n",
    "        return val_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning import Trainer\n",
    "\n",
    "MAX_EPOCHS = 50\n",
    "\n",
    "# pytorch_lightning < 2.0.0\n",
    "# trainer = Trainer(gpus=1, max_epochs=MAX_EPOCHS, gradient_clip_val=0.1, accumulate_grad_batches=8, log_every_n_steps=5)\n",
    "\n",
    "# pytorch_lightning >= 2.0.0\n",
    "trainer = Trainer(\n",
    "    devices=1, \n",
    "    accelerator=\"gpu\",\n",
    "    max_epochs=MAX_EPOCHS, \n",
    "    gradient_clip_val=0.1, \n",
    "    accumulate_grad_batches=8, \n",
    "    log_every_n_steps=5\n",
    ")\n",
    "\n",
    "model = Detr(lr=1e-4, lr_backbone=1e-5, weight_decay=1e-4)\n",
    "\n",
    "trainer.fit(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
